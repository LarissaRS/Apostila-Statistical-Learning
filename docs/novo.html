<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Estatística básica</title>

<script src="site_libs/header-attrs-2.7/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-sm-12 col-md-4 col-lg-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-sm-12 col-md-8 col-lg-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Sobre</a>
</li>
<li>
  <a href="apostilaR.html">R</a>
</li>
<li>
  <a href="novo.html">Estatistica Básica</a>
</li>
<li>
  <a href="Journal.html">Statistical Learning</a>
</li>
<li>
  <a href="apendice.html">Apêndice Geral</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Estatística básica</h1>

</div>


<p>[EM CONSTRUÇÃO]</p>
<p> </p>
<p> </p>
<div id="análise-de-regressão" class="section level1">
<h1><strong>Análise de Regressão</strong></h1>
<p> </p>
<p>Os métodos de análise de regressão formam um conjunto de poderosas ferramentas estatisticas, que estudam a relação entre duas ou mais variaveis. Por ser de facil interpretação, essas ferramentas podem se aplicar nas mais diversas àreas e situações, como por exemplo: faixa salarial e nível de educação, consumo de açúcar e percentual de gordura, quantidade de fertilizante e crescimento da planta, quantidade gasta em publicidade e quantidade de vendas de um produto, consumo de contéudo na TV e faixa etária, etc. Esse conjunto de ferramentas nos permite lidar com os três tópicos mais comuns quando se trata de regressão:</p>
<ul>
<li><strong>Modelagem:</strong> Cria uma equação que descreve a relação entre as variáveis em questão,de forma parcimoniosa;</li>
<li><strong>Covariância:</strong> Estuda a variação entre as variâveis que aparentemente não tem relação entre si;</li>
<li><strong>Predição:</strong> Estima os resultados do modelo para situações incertas.</li>
</ul>
<p>O termo regressão foi criado por Francis Galton no século 19 durante seu estudo sobre a relação entre a altura de pais e filhos, desenvolvido no artigo <a href="https://galton.org/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf"><em>Regression Toward Mediocrity in Hereditary Stature</em></a>. Hoje aplicamos estas tecnicas com o apoio da programação, aqui faremos uso do software RStudio.</p>
<p> </p>
</div>
<div id="regressão-linear-simples" class="section level1">
<h1>1. Regressão Linear Simples</h1>
<p>Nesta sessão estudaremos as técnicas de regresão aplicadas à duas variaveis que relacionam de forma linear, isto é, essa relação pode ser descrita por uma reta.</p>
<p>Vamos dar uma olhada nos <a href="https://people.sc.fsu.edu/~jburkardt/datasets/regression/x09.txt">dados</a> idade e teor de gordura no sangue que seguem abaixo:</p>
<pre><code>## # A tibble: 6 x 2
##   Idade Gordura
##   &lt;dbl&gt;   &lt;dbl&gt;
## 1    20     190
## 2    23     254
## 3    23     181
## 4    24     209
## 5    25     302
## 6    28     288</code></pre>
<p>Como pode-se notar no gráfico, a relação entre idade e teor de gordura no sangue aparentemente linear.</p>
<p><img src="novo_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Sabemos que na matemática básica, a relação linear é descrita como:</p>
<p><span class="math display">\[Y_i = a+bX_i\]</span> onde:<br />
- <span class="math inline">\(Y\)</span> são as <span class="math inline">\(i\)</span> variáveis dependentes;<br />
- <span class="math inline">\(a\)</span> é o intercepto;<br />
- <span class="math inline">\(b\)</span> é o coeficiente angular; - <span class="math inline">\(X\)</span> são as <span class="math inline">\(i\)</span> variáveis independetes;<br />
- <span class="math inline">\(i=1,2,3...\)</span>.</p>
<p>No caso do exemplo, a relação entre Idade e Teor de Gordura no Sangue pode ser descrita por:</p>
<p><span class="math display">\[\hat{y}_i = 102.6 + 5.3x_i\]</span>  </p>
</div>
<div id="modelo" class="section level1">
<h1>1.1 Modelo</h1>
<p>Em regressão linear, a estrutura é quase a mesma. Mudamos o nome dos parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> para as letras gregas <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> respectivamente e adicionamos o termo <span class="math inline">\(\varepsilon_i\)</span>, que vai representar o erro (também chamado de ruído) de cada observação (já que, quando coletamos determinado medida em um experimento, essa medição é passivel de pequenos erros a cada coleta). Então, o <strong>modelo de regressão linear simples</strong> é:</p>
<p><span class="math display">\[Y_i =\beta_0+\beta_1X_i + \varepsilon_i\]</span> onde:</p>
<ul>
<li><span class="math inline">\(Y\)</span> são as <span class="math inline">\(i\)</span> variáveis independentes (ou resposta para a i-ésima obervação);<br />
</li>
<li><span class="math inline">\(\beta_0\)</span> é o intercepto;<br />
</li>
<li><span class="math inline">\(\beta_1\)</span> é o incremento de <span class="math inline">\(X_i\)</span> em <span class="math inline">\(Y_i\)</span>;</li>
<li><span class="math inline">\(X\)</span> são as <span class="math inline">\(i\)</span> variáveis independetes (ou conhecidas);</li>
<li><span class="math inline">\(\varepsilon_i\)</span> são os erros de cada observação <span class="math inline">\(X_i\)</span>;</li>
<li><span class="math inline">\(i=1,2,3...\)</span>.</li>
</ul>
<p>Voltando ao exemplo, na figura abaixo temos as representações dos parâmetros nas partes em destaque: <span class="math inline">\(\beta_0\)</span> como intercepto, isto é, ponto em que a reta corta o eixo <span class="math inline">\(Y\)</span>; <span class="math inline">\(\beta_1\)</span> como incremento em <span class="math inline">\(Y\)</span> para cada uma unidade de <span class="math inline">\(X\)</span>; e por fim <span class="math inline">\(\varepsilon_i\)</span> como a distância entre a reta de regressão e a observação <span class="math inline">\(X_i\)</span>.</p>
<div style="float:center;max-width:90%; max-height: 120%;" markdown="1">
<p><img src="images/denovo.jpeg" /></p>
</div>
<p>Deste modo, nosso modelo de regressão linear simples que descreve a relação entre idade e teor de gordura no sangue é:</p>
<p><span class="math display">\[\hat{y}_i = 102.6 + 5.3x_i+\varepsilon_i\]</span> para <span class="math inline">\(i=1,2,...,24,25\)</span>.</p>
<p> </p>
</div>
<div id="estimação-dos-parâmetros" class="section level1">
<h1>1.2 Estimação dos parâmetros</h1>
<p> </p>
<p>Mas afinal, como determinamos os valores de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span>? Para resolver essa questão, temos dois métodos estatisticos: método de mínimos quadrados e método da máxima verossimilhança. Em ambos o objetivo é sempre encontrar um <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> que nos dê a melhor reta em relação aos dados.</p>
<p> </p>
</div>
<div id="método-de-mínimos-quadrados" class="section level1">
<h1>1.2.1 Método de Mínimos Quadrados</h1>
<p> </p>
<p>Para encontrar a melhor reta, este método minimiza a soma dos erros <span class="math inline">\(\varepsilon_i\)</span>, ou seja, a soma das distâncias entre a reta e os dados coletados. Como estamos somando o tamanho desse erros, elevamos seus valores ao quadrado, então temos:</p>
<p><span class="math display">\[\varepsilon_1^2+\varepsilon_2^2+\varepsilon_3^2+...+\varepsilon_n^2 = \sum_{i=1}^{n} \varepsilon_i^2\]</span></p>
<p>para <span class="math inline">\(i= 1,2,3,...n.\)</span>, i.e. para n observações. Como <span class="math inline">\(\varepsilon_i = Y_i-(\beta_0+\beta_1X_i)= Y_i-\beta_0-\beta_1X_i\)</span>, então queremos encontrar um <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> tal que minimize <span class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[Q=\sum_{i=1}^{n} \varepsilon_i^2=\sum_{i=1}^{n} (Y_i-\beta_0-\beta_1X_i)^2\]</span> Para encontrar essas estimativas analiticamente nos baseando no modelo de regressão simples, usaremos as duas equações que seguem abaixo,conjuntamente:</p>
<p><span class="math display">\[\sum_{i=1}^{n}Y_i = n\beta_0-\beta_1\sum_{i=1}^{n}X_i\]</span> <span class="math display">\[\sum_{i=1}^{n}X_iY_i = \beta_0\sum_{i=1}^{n}X_i-\beta_1\sum_{i=1}^{n}X_i^2\]</span> Essa equações, também chamadas de equações normais, podem ser derivadas em relação aos parâmetros:</p>
<p><span class="math display">\[\frac{\partial Q}{\partial \beta_0}=-2\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)\]</span><br />
<span class="math display">\[\frac{\partial Q}{\partial \beta_1}=-2\sum_{i=1}^{n}X_i(Y_i-\beta_0-\beta_1X_i)\]</span></p>
<p>Igualando essas derivadas à zero, encontramos os valores de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> que minimizam <span class="math inline">\(Q\)</span>:</p>
<p> </p>
<div style="float:right;max-width:35%; max-height: 35%;" markdown="1">
<p><img src="images/note1.jpg" /></p>
</div>
<p><span class="math display">\[-2\sum_{i=1}^{n}(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span> <span class="math display">\[-2\sum_{i=1}^{n}X_i(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span></p>
<p>Dividindo os dois lados das equações por <span class="math inline">\(-2\)</span>: <span class="math display">\[\sum_{i=1}^{n}(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span> <span class="math display">\[\sum_{i=1}^{n}X_i(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span></p>
<p>Expandindo as equações, conseguimos chegar nas equações normais: <span class="math display">\[\sum_{i=1}^{n}Y_i-n\hat{\beta}_0-\hat{\beta}_1\sum_{i=1}^{n}X_i=0\]</span> <span class="math display">\[\sum_{i=1}^{n}X_iY_i-\hat{\beta}_0\sum_{i=1}^{n}X_i-\hat{\beta}_1\sum_{i=1}^{n}X_i^2=0\]</span></p>
<p> </p>
<div class="panel panel-success">
<p><span class="math inline">\(\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\)</span> <strong>Equações Normais</strong></p>
<div class="panel-body">
<p><span class="math display">\[\sum_{i=1}^{n}Y_i\quad=n\hat{\beta}_0+\hat{\beta}_1\sum_{i=1}^{n}X_i\]</span> <span class="math display">\[\sum_{i=1}^{n}X_iY_i\quad=\hat{\beta}_0\sum_{i=1}^{n}X_i+\hat{\beta}_1\sum_{i=1}^{n}X_i^2\]</span></p>
</div>
</div>
<p> </p>
<p>A partir delas conseguim ao isolar os parâmetros e obter as estimações. Para mais detalhes desta etapa, veja a <a href="https://larissars.github.io/Apostilas-estatistica/apendice.html#Demonstração_1">Apêndice: Demonstração 1</a>.</p>
<p> </p>
<p><span class="math display">\[\hat{\beta}_0=\bar{Y}-\beta_1\bar{X}\]</span></p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^{n}(X_i-\bar{X})}\]</span> Considerando: <span class="math inline">\(\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})=S_{xy}\)</span> como a soma dos produtos de X e Y; <span class="math inline">\(\sum_{i=1}^{n}(X_i-\bar{X})=S_{XX}\)</span> como a soma dos quadrados de X. Então podemos dizer que</p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^{n}(X_i-\bar{X})}=\frac{S_{xy}}{S_{xx}}\]</span></p>
<p>As <strong>vantagens</strong> do método dos mínimos quadrados é que, além de ser comumente usado, ele é comportado pelos programas de estatística de modo geral.<br />
Entretanto, precisamos de certos <strong>requisitos</strong> para poder usar ele:</p>
<ul>
<li><strong>Lineariedade dos dados:</strong> seu comportamento pode ser decrito por uma reta</li>
</ul>
<div style="float:center;max-width:90%; max-height: 90%;" markdown="1">
<p><img src="images/regressoes.jpg" /></p>
</div>
<ul>
<li><strong>Normalidade dos resíduos:</strong> os resíduos do modelo seguem uma distribuição aproximadamente normal, i.e., <span class="math inline">\(\epsilon \cong N(\mu, \sigma^2)\)</span>.</li>
</ul>
<div style="float:center;max-width:45%; max-height: 45%;" markdown="1">
<p><img src="images/resinorm.jpg" /></p>
</div>
<ul>
<li><strong>Homocedasticidade:</strong> a variabilidade dos resíduos é constante., ou seja, <span class="math inline">\(Var(\epsilon)=c\)</span>.</li>
</ul>
<div style="float:center;max-width:40%; max-height: 40%;" markdown="1">
<p><img src="images/homoresi.jpg" /></p>
</div>
<ul>
<li><strong>Erros sem autocorrelação:</strong> Os valores ordenados não tem relação com o espaço ou tempo. Matematicamente, <span class="math inline">\(\epsilon_i\)</span> e <span class="math inline">\(\epsilon_j\)</span> tem <span class="math inline">\(Cov(\epsilon_i, \epsilon_j)=0, \forall i \ne j\)</span>.</li>
</ul>
<div style="float:center;max-width:50%; max-height: 50%;" markdown="1">
<p><img src="images/autocor.jpg" /></p>
</div>
</div>
<div id="propriedades" class="section level1">
<h1>1.2.1.1 Propriedades</h1>
<p>Os estimadores <span class="math inline">\(\hat{\beta}\)</span> obtidos pelo método de método de mínimos quadrados são funções lineares de Y (<a href="https://larissars.github.io/Apostilas-estatistica/apendice.html#Demonstração_2">Apêndice: Demonstação 2</a>). E segundo o Teorema de Gauss Markov, dado as condições do modelo de regressão linear, o método de mínimos quadrados:</p>
<ul>
<li><p>Tem estimadores <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> não viesados, i.e., o valor esperado do estimador é ao próprio arametro que foi estimado, por exemplo <span class="math inline">\(E(\hat{\theta})=\theta\)</span>. Isso independente da distribuição de probabilidade desses erros (<a href="https://larissars.github.io/Apostilas-estatistica/apendice.html#Demonstração_3">Apêndice: Demonstação 3</a>).</p></li>
<li><p>Tem a mínima variância entre todos os estimadores não viesados e lineares. Logo eles são os mais precisos entre esse tipo de estimador.</p></li>
</ul>
</div>
<div id="propriedades-do-modelo" class="section level1">
<h1>1.2.1.2 Propriedades do modelo</h1>
</div>
<div id="r²" class="section level1">
<h1>1.2.1.3 R²</h1>
</div>
<div id="exemplo" class="section level1">
<h1>1.2.1.4 Exemplo</h1>
<p> </p>
</div>
<div id="método-da-máxima-verossimilhança" class="section level1">
<h1>1.2.2 Método da Máxima Verossimilhança</h1>
<p>O método de máxima verossilimilhança utiliza o produtos das densidades das distribuição de probabilidade de <span class="math inline">\(Y_i\)</span> como uma medida para a <em>consistencia</em> dos parâmetros para aquela amostra. Assim o método escolhe os valores máximos da verossimilhança estimada, tal que os valores dos paramêtros sejam mais consistentes.</p>
<p>Partindo do fato que <span class="math inline">\(E(Y_i) = \beta_0-\beta_1X_i\)</span> e <span class="math inline">\(Var[Y_i]=\sigma^2\)</span>, então a função densidade de probabilidade de <span class="math inline">\(Y_i\)</span> será:</p>
<p><span class="math display">\[f_i=\frac{1}{\sqrt {2\pi\sigma} }exp\bigg[-\frac{1}{2}\bigg(\frac{Y_i-\beta_0-\beta_1X_i}{\sigma}\bigg)^2 \bigg]\]</span></p>
<p>Fazendo o produtório das n densidades, correspondentes a cada uma das n observações, temos a função de máxima verossimilhança. Nela consideramos que a variância dos erros de cada observação é desconhecida:</p>
<p><span class="math display">\[\mathcal{L}(\beta_o, \beta_1, \sigma^2)=  \prod_{i=1}^{n} \frac{1}{(2\pi\sigma^2)^{1/2}}exp\bigg[-\frac{1}{2\sigma^2}(Y_i-\beta_0-\beta_1X_i)^2 \bigg]\]</span> Simplificando a equação temos: <span class="math display">\[\mathcal{L}(\beta_o, \beta_1, \sigma^2)=\frac{1}{(2\pi\sigma^2)^{n/2}}exp\bigg[ -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2\bigg]\]</span></p>
<p>Para encontrar as estimativas dos parâmetros precisaremos fazer as derivadas parciais de <span class="math inline">\(L(\beta_o, \beta_1, \sigma^2)\)</span> em relação a cada parâmetro. Como <span class="math inline">\(L(\beta_o, \beta_1, \sigma^2)\)</span> e <span class="math inline">\(ln(L(\beta_o, \beta_1, \sigma^2))\)</span> são equações que maximizam a verossimilhança, podemos trabalhar com ambos. Note que usaremos a seguinte notação:</p>
<p><span class="math display">\[L(\beta_o, \beta_1, \sigma^2) = -\frac{n}{2} ln( 2\pi) -\frac{n}{2} ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)^2\]</span></p>
<div style="float:right;max-width:35%; max-height: 35%;" markdown="1">
<p><img src="images/note2.jpg" /></p>
</div>
<p>Mas escolhemos o logaritmo da função de máxima verossimilhança por ser mais facil de derivar. Seguem as derivadas particias dos parâmetros, já igualadas a zero:</p>
<p> </p>
<p><span class="math display">\[\frac{\partial L(\beta_o, \beta_1, \sigma^2)}{\partial \beta_0}\quad= -\frac{1}{\sigma^2}\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)=0\]</span></p>
<p><span class="math display">\[\qquad\qquad\quad=\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)=0\]</span> <span class="math display">\[\sum_{i=1}^{n}y_i=n\beta_0+(\sum_{i=1}^{n}x_i)\beta_1\]</span> <span class="math display">\[\hat{\beta}_0=\bar{Y}-\beta_1\bar{X}\]</span></p>
<p> </p>
<p><span class="math display">\[\frac{\partial L(\beta_o, \beta_1, \sigma^2)}{\partial \beta_1} \quad= -\frac{1}{\sigma^2}\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)=0\]</span> <span class="math display">\[\quad\quad\quad\quad\quad\quad=\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)x_i=0\]</span> <span class="math display">\[\sum_{i=1}^{n}y_ix_i=(\sum_{i=1}^{n}x_i)\beta_0+(\sum_{i=1}^{n}x_i^2)\beta_1  \]</span></p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^{n}(X_i-\bar{X})}\]</span></p>
<p> </p>
<p><span class="math display">\[\frac{\partial L(\beta_o, \beta_1, \sigma^2)}{\partial \sigma^2}=\frac{n}{\hat{\sigma}^2}-\frac{1}{\hat{\sigma}^4}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)=0\]</span></p>
<p><span class="math display">\[\frac{n}{\hat{\sigma}^2}=\frac{1}{\hat{\sigma}^4}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)\]</span> <span class="math display">\[n=\frac{1}{\hat{\sigma}^2}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)\]</span></p>
<p><span class="math display">\[\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2\]</span> Como <span class="math inline">\(\hat{Y}_i=\beta_0-\beta_1X_i\)</span>, então:</p>
<p><span class="math display">\[\hat{\sigma}^2=\frac{\sum_{i=1}^{n}(Y_i-\hat{Y}_i)^2}{n}\]</span></p>
<p>Os dois metodos juntos (pah 721)</p>
</div>
<div id="análise-de-resíduos-openintro" class="section level1">
<h1>1.3 Análise de Resíduos (Openintro)</h1>
<p>a)analise de residuos a1) pq fazer a2) como fazer</p>
<div id="análise-de-variância" class="section level3">
<h3>Análise de Variância</h3>
<p>a)pq fazer b)exemplos c)como fazer</p>
</div>
<div id="intervalos-de-confiança" class="section level3">
<h3>Intervalos de confiança</h3>
<pre><code>  a)  exemplos de uso
  b) como fazer</code></pre>
</div>
<div id="teste-de-hipótese" class="section level3">
<h3>Teste de Hipótese</h3>
<pre><code>  a)  exemplos de uso
  b) como fazer</code></pre>
</div>
<div id="predição" class="section level3">
<h3>Predição</h3>
<pre><code>  a)  exemplos de uso
  b) como fazer</code></pre>
</div>
<div id="diagnostico" class="section level3">
<h3>Diagnostico</h3>
<p>a)pq fazer b)como fazer ### Transformações a)pq fazer b)exemplos c)como fazer</p>
<p> </p>
</div>
<div id="regressão-linear-multipla" class="section level2">
<h2>Regressão Linear Multipla</h2>
<div id="forma-matricial" class="section level3">
<h3>Forma matricial</h3>
</div>
<div id="estimação-dos-parâmetros-1" class="section level3">
<h3>Estimação dos parâmetros</h3>
</div>
<div id="análise-de-variância-1" class="section level3">
<h3>Análise de Variância</h3>
</div>
<div id="predição-1" class="section level3">
<h3>Predição</h3>
</div>
<div id="teste-de-hipótese-1" class="section level3">
<h3>Teste de Hipótese</h3>
</div>
<div id="intervalo-de-confiança" class="section level3">
<h3>Intervalo de Confiança</h3>
<p>Soma extra de quadrados(?)<br />
Coef de determinação parcial (?)<br />
Reg multi padronizada (?)<br />
Multicolineariedade(?)<br />
Medidas de influência e Alavancagem (?) *Caio</p>
<p> </p>
</div>
</div>
<div id="regressão-polinomial" class="section level2">
<h2>Regressão Polinomial</h2>
<p>Regressão com intervenção (?)<br />
Região de confiança (?)<br />
Teste de Hip Linear (?)</p>
<p> </p>
</div>
<div id="seleção-de-modelos" class="section level2">
<h2>Seleção de modelos</h2>
<p>Regressão Parcial (?)<br />
Inflação da variância (?)</p>
<p> </p>
</div>
<div id="regressão-não-linear" class="section level2">
<h2>Regressão Não-Linear</h2>
<p>Regressão logistica(?) *openintro</p>
</div>
</div>
<div id="referência" class="section level1">
<h1>Referência</h1>
<p> </p>
<ul>
<li>Análise de Regressão</li>
</ul>
<p><strong>Azevedo, Caio L. N.</strong>. ME 613A - Análise de regressão, Primeiro Semestre 2019. IMECC - UNICAMP. Disponível em: <a href="https://www.ime.unicamp.br/~cnaber/Material_ME613_1S_2019.htm">https://www.ime.unicamp.br/~cnaber/Material_ME613_1S_2019.htm</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Carvalho, B</strong>. ME613: Análise de Regressão. Github. 2021. Disponível em: <a href="http://me613-unicamp.github.io/">http://me613-unicamp.github.io/</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Caffo, B</strong>. (2019). Regression models for data science in R. Leanpub. 2019. Disponível em: <a href="https://leanpub.com/regmods/read">https://leanpub.com/regmods/read</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Diez, D.; Rundel, M. C.; Barr, C. D.</strong>. Openintro Statistics. Quarta edição. Atualizado em 12 de Novembro de 2019. Versão online gratuita. Disponível em: <a href="https://www.openintro.org/book/os/">https://www.openintro.org/book/os/</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Kutner, M. H.; Nachtsheim, C. J.;Neter, J.; Li, W.</strong>.Applied Linear Statistical Models. Quinta edição. 2005.</p>
<p>João L. F. Batista. Análise de Regressão Aplicada.2004. Departamento de Ciências Florestais ESALQ - USP</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
