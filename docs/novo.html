<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Estatística básica</title>

<script src="site_libs/header-attrs-2.7/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-sm-12 col-md-4 col-lg-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-sm-12 col-md-8 col-lg-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Sobre</a>
</li>
<li>
  <a href="apostilaR.html">R</a>
</li>
<li>
  <a href="novo.html">Estatistica Básica</a>
</li>
<li>
  <a href="Journal.html">Statistical Learning</a>
</li>
<li>
  <a href="apendice.html">Apêndice Geral</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Estatística básica</h1>

</div>


<p>[EM CONSTRUÇÃO]</p>
<p> </p>
<p> </p>
<div id="análise-de-regressão" class="section level1">
<h1>Análise de Regressão</h1>
<p> </p>
<p>Os métodos de análise de regressão formam um conjunto de poderosas ferramentas estatisticas, que estudam a relação entre duas ou mais variaveis. Por ser de facil interpretação, essas ferramentas podem se aplicar nas mais diversas àreas e situações, como por exemplo: faixa salarial e nível de educação, consumo de açúcar e percentual de gordura, quantidade de fertilizante e crescimento da planta, quantidade gasta em publicidade e quantidade de vendas de um produto, consumo de contéudo na TV e faixa etária, etc. Esse conjunto de ferramentas nos permite lidar com os três tópicos mais comuns quando se trata de regressão:</p>
<ul>
<li><strong>Modelagem:</strong> Cria uma equação que descreve a relação entre as variáveis em questão,de forma parcimoniosa;</li>
<li><strong>Covariância:</strong> Estuda a variação entre as variâveis que aparentemente não tem relação entre si;</li>
<li><strong>Predição:</strong> Estima os resultados do modelo para situações incertas.</li>
</ul>
<p>O termo regressão foi criado por Francis Galton no século 19 durante seu estudo sobre a relação entre a altura de pais e filhos, desenvolvido no artigo <a href="https://galton.org/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf"><em>Regression Toward Mediocrity in Hereditary Stature</em></a>. Hoje aplicamos estas tecnicas com o apoio da programação, aqui faremos uso do software RStudio.</p>
<p> </p>
</div>
<div id="regressão-linear-simples" class="section level1">
<h1>1. Regressão Linear Simples</h1>
<p>Nesta sessão estudaremos as técnicas de regresão aplicadas à duas variaveis que relacionam de forma linear, isto é, essa relação pode ser descrita por uma reta.</p>
<p>Vamos dar uma olhada nos <a href="https://people.sc.fsu.edu/~jburkardt/datasets/regression/x09.txt">dados</a> idade e teor de gordura no sangue que seguem abaixo:</p>
<pre><code>## # A tibble: 6 x 2
##   Idade Gordura
##   &lt;dbl&gt;   &lt;dbl&gt;
## 1    46     354
## 2    20     190
## 3    52     405
## 4    30     263
## 5    57     451
## 6    25     302</code></pre>
<p>Como pode-se notar no gráfico, a relação entre idade e teor de gordura no sangue aparentemente linear.</p>
<p><img src="novo_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Sabemos que na matemática básica, a relação linear é descrita como:</p>
<p><span class="math display">\[Y_i = a+bX_i\]</span> onde:<br />
- <span class="math inline">\(Y\)</span> são as <span class="math inline">\(i\)</span> variáveis dependentes;<br />
- <span class="math inline">\(a\)</span> é o intercepto;<br />
- <span class="math inline">\(b\)</span> é o coeficiente angular; - <span class="math inline">\(X\)</span> são as <span class="math inline">\(i\)</span> variáveis independetes;<br />
- <span class="math inline">\(i=1,2,3...\)</span>.</p>
<p>No caso do exemplo, a relação entre Idade e Teor de Gordura no Sangue pode ser descrita por:</p>
<p><span class="math display">\[\hat{y}_i = 102.6 + 5.3x_i\]</span>  </p>
</div>
<div id="modelo" class="section level1">
<h1>1.1 Modelo</h1>
<p>Em regressão linear, a estrutura é quase a mesma. Mudamos o nome dos parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> para as letras gregas <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> respectivamente e adicionamos o termo <span class="math inline">\(\varepsilon_i\)</span>, que vai representar o erro (também chamado de ruído) de cada observação (já que, quando coletamos determinado medida em um experimento, essa medição é passivel de pequenos erros a cada coleta). Então, o <strong>modelo de regressão linear simples</strong> é:onde:</p>
<ul>
<li><span class="math inline">\(Y\)</span> são as <span class="math inline">\(i\)</span> variáveis independentes (ou resposta para a i-ésima obervação);<br />
</li>
<li><span class="math inline">\(\beta_0\)</span> é o intercepto;<br />
</li>
<li><span class="math inline">\(\beta_1\)</span> é o incremento de <span class="math inline">\(X_i\)</span> em <span class="math inline">\(Y_i\)</span>;</li>
<li><span class="math inline">\(X\)</span> são as <span class="math inline">\(i\)</span> variáveis independetes (ou conhecidas);</li>
<li><span class="math inline">\(\varepsilon_i\)</span> são os erros aleatórios associados a cada observação <span class="math inline">\(X_i\)</span>;</li>
<li><span class="math inline">\(i=1,2,3...\)</span>.</li>
</ul>
<p><span class="math display">\[Y_i =\beta_0+\beta_1X_i + \varepsilon_i\]</span></p>
<p>Quando aplicamos este modelo aos nosso dados, teremos os <strong>valores ajustados</strong> ou <strong>estimados</strong>, i.e.:</p>
<p><span class="math display">\[\hat{Y_i}= \hat{\beta_0}+\hat{\beta_1}X_i\]</span> Nestes valores ajustados estão contidos os <strong>resíduos</strong>, que são a diferença entre o valor verdadeiro da variável resposta e o valor estimado pelo modelo, respectivamente:</p>
<p><span class="math display">\[\epsilon_i=Y_i-\hat{Y_i}\]</span> <span class="math display">\[\qquad\qquad\quad=Y_i-\hat{\beta_0}+\hat{\beta_1}X_i\]</span></p>
<p>Vale lembrar que <strong>erro</strong> e <strong>resíduo</strong> não são a mesma coisa:</p>
<ul>
<li>O erro aleatório <span class="math inline">\(\varepsilon_i\)</span> do modelo se refere ao contexto populacional, antes do ajuste do modelo e amostragem.</li>
<li>O resíduo <span class="math inline">\(\epsilon_i\)</span> se refere ao contexto amostral, após o modelo ser ajustado com base na amostra coletada.</li>
</ul>
<p>A primeira suposição a respeito deste modelo a respeito dos erros:</p>
<ul>
<li>O valor esperado dos erros é sempre zero: <span class="math inline">\(E[\varepsilon_i]=0 \quad\forall i\)</span>;<br />
</li>
<li>Sua variância é sempre constante <span class="math inline">\(Var[\varepsilon_i]=\sigma^2\quad\forall i\)</span>;<br />
</li>
<li>Eles não são correlacionados: <span class="math inline">\(Cov(\varepsilon_i, \varepsilon_j)=0 \quad \forall\quad i \ne j\)</span>.</li>
<li>Eles seguem a distribuição de probabilidade normal e são independentes e identicamente distribuidos: <span class="math inline">\(\varepsilon \stackrel{iid}{\sim} N(0, \sigma^2)\)</span></li>
</ul>
<div style="float:centre; max-width:100%; max-height: 80%;" markdown="1">
<p><img src="images/note4.png" /></p>
</div>
<p>Voltando ao exemplo, na figura abaixo temos as representações dos parâmetros nas partes em destaque: <span class="math inline">\(\beta_0\)</span> como intercepto, isto é, ponto em que a reta corta o eixo <span class="math inline">\(Y\)</span>; <span class="math inline">\(\beta_1\)</span> como incremento em <span class="math inline">\(Y\)</span> para cada uma unidade de <span class="math inline">\(X\)</span>; e por fim <span class="math inline">\(\varepsilon_i\)</span> como a distância entre a reta de regressão e a observação <span class="math inline">\(X_i\)</span>.</p>
<div style="float:center;max-width:90%; max-height: 120%;" markdown="1">
<p><img src="images/denovo.jpeg" /></p>
</div>
<p>Deste modo, nosso modelo de regressão linear simples que descreve a relação entre idade e teor de gordura no sangue é:</p>
<p><span class="math display">\[\hat{y}_i = 102.6 + 5.3x_i+\varepsilon_i\]</span> para <span class="math inline">\(i=1,2,...,24,25\)</span>.</p>
<p> </p>
</div>
<div id="propriedades-do-modelo" class="section level1">
<h1>1.2 Propriedades do modelo</h1>
<p>Agora que conhecemos o modelo, vamos conferir algumas de suas propriedades. Sabemos então qe <span class="math inline">\(Y_i\)</span> são as variáveis resposta e que <span class="math inline">\(\varepsilon_i\)</span> são variáveis aleatórias correspondentes aos erros. Como o valor esperado de <span class="math inline">\(\varepsilon_i\)</span> é sempre zero, então:</p>
<div style="float:right;max-width:50%; max-height: 70%;" markdown="1">
<p><img src="images/note5.png" /></p>
</div>
<p><span class="math display">\[E[Y_i]=E[\beta_0+\beta_1X_i+\varepsilon_i]\]</span> <span class="math display">\[\qquad\quad=E[\beta_0+\beta_1X_i]+E[\varepsilon_i]\]</span> <span class="math display">\[\qquad\quad=E[\beta_0+\beta_1X_i]+0\quad\]</span> <span class="math display">\[\qquad=\beta_0+\beta_1X_i\quad\qquad\]</span></p>
<p> </p>
<p><span class="math inline">\(\qquad\qquad Var[Y_i]=Var[\beta_0+\beta_1X_i+\varepsilon_i]\)</span></p>
<p><span class="math inline">\(\qquad\qquad\qquad\quad = Var[\varepsilon_i]\qquad\qquad\)</span></p>
<p><span class="math inline">\(\qquad\qquad\qquad\quad = \sigma^2\qquad\qquad\)</span></p>
<p>Além disso, como os erros <span class="math inline">\(\varepsilon_i\)</span> e <span class="math inline">\(\varepsilon_j\)</span> não correlacionados, então as respostas <span class="math inline">\(Y_i\)</span> e <span class="math inline">\(Y_j\)</span> também não serão.</p>
</div>
<div id="estimação-dos-parâmetros" class="section level1">
<h1>1.2 Estimação dos parâmetros</h1>
<p> </p>
<p>Mas afinal, como determinamos os valores de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span>? Para resolver essa questão, temos dois métodos estatisticos: método de mínimos quadrados e método da máxima verossimilhança. Em ambos o objetivo é sempre encontrar um <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> que nos dê a melhor reta em relação aos dados.</p>
<p> </p>
</div>
<div id="método-de-mínimos-quadrados-mmq" class="section level1">
<h1>1.2.1 Método de Mínimos Quadrados (MMQ)</h1>
<p> </p>
<p>Para encontrar a melhor reta, este método minimiza a soma dos erros <span class="math inline">\(\varepsilon_i\)</span>, ou seja, a soma das distâncias entre a reta e os dados coletados. Como estamos somando o tamanho desse erros, elevamos seus valores ao quadrado, então temos:</p>
<p><span class="math display">\[\varepsilon_1^2+\varepsilon_2^2+\varepsilon_3^2+...+\varepsilon_n^2 = \sum_{i=1}^{n} \varepsilon_i^2\]</span></p>
<p>para <span class="math inline">\(i= 1,2,3,...n.\)</span>, i.e. para n observações. Como <span class="math inline">\(\varepsilon_i = Y_i-(\beta_0+\beta_1X_i)= Y_i-\beta_0-\beta_1X_i\)</span>, então queremos encontrar um <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> tal que minimize <span class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[Q=\sum_{i=1}^{n} \varepsilon_i^2=\sum_{i=1}^{n} (Y_i-\beta_0-\beta_1X_i)^2\]</span> Para encontrar essas estimativas analiticamente nos baseando no modelo de regressão simples, usaremos as duas equações que seguem abaixo,conjuntamente:</p>
<p><span class="math display">\[\sum_{i=1}^{n}Y_i = n\beta_0-\beta_1\sum_{i=1}^{n}X_i\]</span> <span class="math display">\[\sum_{i=1}^{n}X_iY_i = \beta_0\sum_{i=1}^{n}X_i-\beta_1\sum_{i=1}^{n}X_i^2\]</span> Essa equações, também chamadas de equações normais, podem ser derivadas em relação aos parâmetros:</p>
<p><span class="math display">\[\frac{\partial Q}{\partial \beta_0}=-2\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)\]</span><br />
<span class="math display">\[\frac{\partial Q}{\partial \beta_1}=-2\sum_{i=1}^{n}X_i(Y_i-\beta_0-\beta_1X_i)\]</span></p>
<p>Igualando essas derivadas à zero, encontramos os valores de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> que minimizam <span class="math inline">\(Q\)</span>:</p>
<p> </p>
<div style="float:right;max-width:35%; max-height: 35%;" markdown="1">
<p><img src="images/note1.jpg" /></p>
</div>
<p><span class="math display">\[-2\sum_{i=1}^{n}(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span> <span class="math display">\[-2\sum_{i=1}^{n}X_i(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span></p>
<p>Dividindo os dois lados das equações por <span class="math inline">\(-2\)</span>: <span class="math display">\[\sum_{i=1}^{n}(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span> <span class="math display">\[\sum_{i=1}^{n}X_i(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\]</span></p>
<p>Expandindo as equações, conseguimos chegar nas equações normais: <span class="math display">\[\sum_{i=1}^{n}Y_i-n\hat{\beta}_0-\hat{\beta}_1\sum_{i=1}^{n}X_i=0\]</span> <span class="math display">\[\sum_{i=1}^{n}X_iY_i-\hat{\beta}_0\sum_{i=1}^{n}X_i-\hat{\beta}_1\sum_{i=1}^{n}X_i^2=0\]</span></p>
<p> </p>
<div class="panel panel-success">
<p><span class="math inline">\(\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\)</span> <strong>Equações Normais</strong></p>
<div class="panel-body">
<p><span class="math display">\[\sum_{i=1}^{n}Y_i\quad=n\hat{\beta}_0+\hat{\beta}_1\sum_{i=1}^{n}X_i\]</span> <span class="math display">\[\sum_{i=1}^{n}X_iY_i\quad=\hat{\beta}_0\sum_{i=1}^{n}X_i+\hat{\beta}_1\sum_{i=1}^{n}X_i^2\]</span></p>
</div>
</div>
<p> </p>
<p>A partir delas conseguim ao isolar os parâmetros e obter as estimações. Para mais detalhes desta etapa, veja a <a href="https://larissars.github.io/Apostilas-estatistica/apendice.html#Demonstração_1">Apêndice: Demonstração 1</a>.</p>
<p> </p>
<p><span class="math display">\[\hat{\beta}_0=\bar{Y}-\beta_1\bar{X}\]</span></p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^{n}(X_i-\bar{X})}\]</span></p>
<p>As <strong>vantagens</strong> do método dos mínimos quadrados é que, além de ser comumente usado, ele é comportado pelos programas de estatística de modo geral.<br />
Entretanto, precisamos de certos <strong>requisitos</strong> para poder usar ele:</p>
<ul>
<li><strong>Lineariedade dos dados:</strong> seu comportamento pode ser decrito por uma reta</li>
</ul>
<div style="float:center;max-width:90%; max-height: 90%;" markdown="1">
<p><img src="images/regressoes.jpg" /></p>
</div>
<ul>
<li><strong>Normalidade dos resíduos:</strong> os resíduos do modelo seguem uma distribuição aproximadamente normal, i.e., <span class="math inline">\(\epsilon \cong N(\mu, \sigma^2)\)</span>.</li>
</ul>
<div style="float:center;max-width:45%; max-height: 45%;" markdown="1">
<p><img src="images/resinorm.jpg" /></p>
</div>
<ul>
<li><strong>Homocedasticidade:</strong> a variabilidade dos resíduos é constante., ou seja, <span class="math inline">\(Var(\epsilon)=c\)</span>.</li>
</ul>
<div style="float:center;max-width:40%; max-height: 40%;" markdown="1">
<p><img src="images/homoresi.jpg" /></p>
</div>
<ul>
<li><strong>Erros sem autocorrelação:</strong> Os valores ordenados não tem relação com o espaço ou tempo. Matematicamente, <span class="math inline">\(\epsilon_i\)</span> e <span class="math inline">\(\epsilon_j\)</span> tem <span class="math inline">\(Cov(\epsilon_i, \epsilon_j)=0, \forall i \ne j\)</span>.</li>
</ul>
<div style="float:center;max-width:50%; max-height: 50%;" markdown="1">
<p><img src="images/autocor.jpg" /></p>
</div>
</div>
<div id="propriedades-dos-estimadores-de-mmq" class="section level1">
<h1>1.2.1.1 Propriedades dos estimadores de MMQ</h1>
<p>Os estimadores <span class="math inline">\(\hat{\beta}\)</span> obtidos pelo método de método de mínimos quadrados são funções lineares de Y (<a href="https://larissars.github.io/Apostilas-estatistica/apendice.html#Demonstração_2">Apêndice: Demonstação 2</a>). E segundo o Teorema de Gauss Markov, dado as condições do modelo de regressão linear, o método de mínimos quadrados:</p>
<ul>
<li><p>Tem estimadores <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> não viesados, i.e., o valor esperado do estimador é ao próprio arametro que foi estimado: <span class="math inline">\(E[\hat{\beta_0}]= \beta_0\)</span> e <span class="math inline">\(E[\hat{\beta_1}]= \beta_1\)</span>. Isso acontece independente da distribuição de probabilidade desses erros (<a href="https://larissars.github.io/Apostilas-estatistica/apendice.html#Demonstração_3">Apêndice: Demonstação 3</a>).</p></li>
<li><p>Tem variância mínima entre todos os estimadores não viesados e lineares.</p></li>
</ul>
<p><span class="math display">\[Var[\hat{\beta_0}]=\sigma^2\Bigg[\frac{1}{n}+\frac{\bar{X}^2}{\sum_{i=0}^{n}(X_i-\bar{X}^2)}\Bigg]\]</span></p>
<p><span class="math display">\[Var[\hat{\beta_1}]=\frac{\sigma^2}{\sum_{i=0}^{n}(X_i-\bar{X})^2}\]</span></p>
<p>Logo eles são os mais precisos entre esse tipo de estimador.</p>
<ul>
<li>Os estimadores de mínimos quadrados tem distribuição de probabilidade Normal.</li>
</ul>
<p><span class="math display">\[\hat{\beta_0} \sim N\Bigg(\beta_0, \sigma^2\Bigg[\frac{1}{n}+\frac{\bar{X}^2}{\sum_{i=0}^{n}(X_i-\bar{X}^2)}\Bigg]\Bigg)\]</span></p>
<p><span class="math display">\[\hat{\beta_0} \sim N\Bigg(\beta_1,\frac{\sigma^2}{\sum_{i=0}^{n}(X_i-\bar{X}^2)} \Bigg)\]</span></p>
<p>Além disso, temos que:</p>
<ul>
<li>Considerando <span class="math inline">\(\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})=S_{xy}\)</span> como a soma dos produtos de X e Y, <span class="math inline">\(\sum_{i=1}^{n}(X_i-\bar{X})=S_{XX}\)</span> como a soma dos quadrados de X, podemos afirmar que:</li>
</ul>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^{n}(X_i-\bar{X})}=\frac{S_{xy}}{S_{xx}}\]</span></p>
<ul>
<li><p>Esses estimadores são funções lineares das observações das variáveis resposta <span class="math inline">\(y_i\)</span>.</p></li>
<li><p>Suas variâncias são proporcionais as variâncias dos erros</p></li>
<li><p>São estimadores correlacionados</p></li>
</ul>
<p>Veja mais detalhes sobre os resultados acima em <a href="">Demonstração X</a>.</p>
</div>
<div id="exemplo-no-r" class="section level1">
<h1>1.2.1.3 Exemplo no R</h1>
<p>Para reproduzir o exemplo exibido no inicio do capítulo, precisaremos carregar os pacotes <em>readr</em> e <em>ggplot2</em>.</p>
<pre class="r"><code>#carregando os pacotes necessários
library(&quot;readr&quot;)
library(&quot;ggplot2&quot;)</code></pre>
<p>Em seguidas vamos ler os dados obtidos na <a href="https://people.sc.fsu.edu/~jburkardt/datasets/regression/x09.txt">página</a>.</p>
<pre class="r"><code>dados = read_table2(&quot;https://people.sc.fsu.edu/~jburkardt/datasets/regression/x09.txt&quot;, col_names = FALSE ,skip = 36) #lendo apenas a tabela de dados 

dados = dados[,c(4,5)]#seleciona as duas ultimas colunas

colnames(dados) = c(&quot;Idade&quot;, &quot;Gordura&quot;) #renomendo as colunas

head(dados) #Mostra as 6 primeiras linhas da tabela</code></pre>
<pre><code>## # A tibble: 6 x 2
##   Idade Gordura
##   &lt;dbl&gt;   &lt;dbl&gt;
## 1    46     354
## 2    20     190
## 3    52     405
## 4    30     263
## 5    57     451
## 6    25     302</code></pre>
<p>Com os dados já separados, ajustaremos um modelo de regressão linear simples.</p>
<pre class="r"><code>ajuste &lt;- lm(Gordura ~ Idade, dados) #ajuste do modelo
summary(ajuste) #exibe os resultados detalhados do ajuste</code></pre>
<pre><code>## 
## Call:
## lm(formula = Gordura ~ Idade, data = dados)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -63.478 -26.816  -3.854  28.315  90.881 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 102.5751    29.6376   3.461  0.00212 ** 
## Idade         5.3207     0.7243   7.346 1.79e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 43.46 on 23 degrees of freedom
## Multiple R-squared:  0.7012, Adjusted R-squared:  0.6882 
## F-statistic: 53.96 on 1 and 23 DF,  p-value: 1.794e-07</code></pre>
<p>Dado dos coeficientes do ajuste, ambos significativos, temos o modelo</p>
<p><span class="math display">\[\hat{y}_i = 102.6 + 5.3x_i\]</span></p>
<p>E partir dele, podemos fazer o gráfico da reta dos valores preditos sobre os valores observados.</p>
<p><img src="novo_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p> </p>
</div>
<div id="método-da-máxima-verossimilhança-mmv" class="section level1">
<h1>1.2.2 Método da Máxima Verossimilhança (MMV)</h1>
<div style="float:right;max-width:50%; max-height: 50%;" markdown="1">
<img src="images/note7.png" />
</div>
<p>O método de máxima verossilimilhança utiliza o produtos das densidades das distribuição de probabilidade de <span class="math inline">\(Y_i\)</span> como uma medida para a consistência dos parâmetros para aquela amostra. Assim o método escolhe os valores máximos da verossimilhança estimada, tal que os valores dos paramêtros sejam mais consistentes.</p>
<p>Aqui, por ser mais simples, usaremos a log-verossimilhança negartiva. Partindo do fato que <span class="math inline">\(E(Y_i) = \beta_0-\beta_1X_i\)</span> e <span class="math inline">\(Var[Y_i]=\sigma^2\)</span>, então a função densidade de probabilidade de <span class="math inline">\(Y_i\)</span> será:</p>
<p><span class="math display">\[f_i=\frac{1}{\sqrt {2\pi\sigma} }exp\bigg[-\frac{1}{2}\bigg(\frac{Y_i-\beta_0-\beta_1X_i}{\sigma}\bigg)^2 \bigg]\]</span></p>
<p>Fazendo o produtório das n densidades, correspondentes a cada uma das n observações, temos a função de máxima verossimilhança. Nela consideramos que a variância dos erros de cada observação é desconhecida:</p>
<span class="math display">\[\mathcal{L}(\beta_o, \beta_1, \sigma^2)=  \prod_{i=1}^{n} \frac{1}{(2\pi\sigma^2)^{1/2}}exp\bigg[-\frac{1}{2\sigma^2}(Y_i-\beta_0-\beta_1X_i)^2 \bigg]\]</span> Simplificando a equação temos: <span class="math display">\[\mathcal{L}(\beta_o, \beta_1, \sigma^2)=\frac{1}{(2\pi\sigma^2)^{n/2}}exp\bigg[ -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2\bigg]\]</span>
<div style="float:right;max-width:35%; max-height: 35%;" markdown="1">
<p><img src="images/note2.jpg" /></p>
</div>
<p>Para encontrar as estimativas dos parâmetros precisaremos fazer as derivadas parciais de <span class="math inline">\(L(\beta_o, \beta_1, \sigma^2)\)</span> em relação a cada parâmetro. Como <span class="math inline">\(L(\beta_o, \beta_1, \sigma^2)\)</span> e <span class="math inline">\(ln(\mathcal{L}(\beta_o, \beta_1, \sigma^2))\)</span> são equações que maximizam a verossimilhança, podemos trabalhar com ambos. Note que usaremos a seguinte notação:</p>
<p><span class="math display">\[L(\beta_o, \beta_1, \sigma^2) = -\frac{n}{2} ln( 2\pi) -\frac{n}{2} ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)^2\]</span></p>
<p>Mas escolhemos o logaritmo da função de máxima verossimilhança por ser mais facil de derivar. Seguem as derivadas particias dos parâmetros, já igualadas a zero:</p>
<p> </p>
<p><span class="math display">\[\frac{\partial L(\beta_o, \beta_1, \sigma^2)}{\partial \beta_0}\quad= -\frac{1}{\sigma^2}\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)=0\]</span></p>
<p><span class="math display">\[\quad\quad=\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)=0\]</span> <span class="math display">\[\sum_{i=1}^{n}y_i=n\beta_0+(\sum_{i=1}^{n}x_i)\beta_1\]</span> <span class="math display">\[\hat{\beta}_0=\bar{Y}-\beta_1\bar{X}\]</span></p>
<p> </p>
<p><span class="math display">\[\frac{\partial L(\beta_o, \beta_1, \sigma^2)}{\partial \beta_1} \quad= -\frac{1}{\sigma^2}\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)=0\]</span> <span class="math display">\[\quad\quad\quad\quad\quad\quad=\sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)x_i=0\]</span> <span class="math display">\[\sum_{i=1}^{n}y_ix_i=(\sum_{i=1}^{n}x_i)\beta_0+(\sum_{i=1}^{n}x_i^2)\beta_1  \]</span></p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^{n}(X_i-\bar{X})}\]</span></p>
<p> </p>
<p><span class="math display">\[\frac{\partial L(\beta_o, \beta_1, \sigma^2)}{\partial \sigma^2}=\frac{n}{\hat{\sigma}^2}-\frac{1}{\hat{\sigma}^4}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)=0\]</span></p>
<p><span class="math display">\[\frac{n}{\hat{\sigma}^2}=\frac{1}{\hat{\sigma}^4}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)\]</span> <span class="math display">\[n=\frac{1}{\hat{\sigma}^2}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)\]</span></p>
<p><span class="math display">\[\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2\]</span> Como <span class="math inline">\(\hat{Y}_i=\beta_0-\beta_1X_i\)</span>, então:</p>
<p><span class="math display">\[\hat{\sigma}^2=\frac{\sum_{i=1}^{n}(Y_i-\hat{Y}_i)^2}{n}\]</span></p>
</div>
<div id="propriedades-dos-estimadores-de-mmv" class="section level1">
<h1>1.2.2.1 Propriedades dos estimadores de (MMV)</h1>
<p>Esse estimador é do tipo um estimador viesado para p parâmetro da variância <span class="math inline">\(E[\hat{\sigma^2}]=\frac{n-1}{n}\sigma^2\)</span>. Mas medida que n cresce, <span class="math inline">\(\hat{\sigma^2}\)</span> tende a ser não viesado:</p>
<p><span class="math display">\[lim_{n \to \infty }E[\hat{\sigma^2}]=lim_{n \to \infty} \frac{n-1}{n}\sigma^2=\sigma^2\]</span></p>
<p>Contudo ele é muito confiavel devido algumas propriedades:</p>
<ul>
<li>Consistência;</li>
<li>Eficiência Assintótica;</li>
<li>Normalidade Assimptótica;</li>
<li>Invariância.</li>
</ul>
</div>
<div id="análise-de-resíduos" class="section level1">
<h1>1.3 Análise de Resíduos</h1>
<p>Como já citado anteriormente, os resíduos são a diferença entre o valor verdadeiro da variável resposta e o valor estimado pelo modelo, respectivamente:</p>
<p><span class="math display">\[\epsilon_i=Y_i-\hat{Y_i}\]</span> Eles são usados para avaliar o quão bom é o ajuste do modelo, já que, toda variabilidade não explicada pelo modelo, vai para os resíduos. Para isso vamos verificar seis caracteristicas:</p>
<ul>
<li>Se a regressão ajustada é uma função linear;</li>
<li>Se os erros tem variância constante;</li>
<li>Se os erros são independentes;</li>
<li>Se os erros tem distribuição normal;</li>
<li>Se o modelo não se ajusta para alguns outliers;</li>
<li>Se uma ou mais variaveis preditoras podem ser retiradas do modelo.</li>
</ul>
<p>Um dos gráficos mais usados para avaliar se a função é linear e foi bem ajustada é o gráfico de pontos dos resíduos pelos valores ajustados:</p>
<div style="float:center;max-width:90%; max-height: 90%;" markdown="1">
<p><img src="images/residuos1.png" /></p>
</div>
<p>Em um ajuste satisfatório teremos pontos aleatórios em torno da reta horizontal (que marca o resíduo igual a zero) e que variam de forma constante, como na figura (a). Quando isso não acontece (como nas demais imagens), temos resíduos viesados, os quais podem nos dar pistas sobre o comportamento não captado pelo modelo ajustado. Por exemplo, na figura (b) temos uma comportamento claramento quadrático e na figura (c) uma tendência linear positiva, essas caracteristicas não devem aparecer nos resíduos e sim no modelo ajustado. Se você se deparar com essa situação, refaça seu modelo.</p>
<div style="float:center;max-width:100%; max-height: 100%;" markdown="1">
<p><img src="images/residuos2.png" /></p>
</div>
<p>Além disso é importante lembrar que, quando o valor de um i-ésimo resíduo <span class="math inline">\(\epsilon_i\)</span> for:</p>
<ul>
<li>Positivo, significa que o modelo subestimou a observação</li>
</ul>
<p><span class="math display">\[\epsilon_i&gt;0 \quad \to \quad Y_i-\hat{Y_i}&gt;0 \quad \to \quad Y_i&gt;\hat{Y_i}\]</span></p>
<ul>
<li>Negativo, significa que o modelo superestimou a observação</li>
</ul>
<p><span class="math display">\[\epsilon_i&lt;0 \quad \to \quad Y_i-\hat{Y_i}&lt;0 \quad \to \quad Y_i&lt;\hat{Y_i}\]</span> Outro gráfico importante é o dos resíduos absolutos (ou residuos ao quadrado) versus a variável X (preditora) ou os valores ajustados de Y (i.e. <span class="math inline">\(\hat Y\)</span>). Este gráfico é util para checarmos a variância dos resíduos é ou não constante, bem como a presença de outliers.</p>
<div style="float:center;max-width:90%; max-height: 90%;" markdown="1">
<p><img src="images/residuos4.JPG" /></p>
</div>
<p>Já a independência dos erros pode ser avaliada pelo simples gráfico de pontos dos resíduos em relação a variável preditora ou em relação ao tempo, pois com eles conseguimos ver se há a relação de um i-ésimo resíduo com os demais próximos a ele.</p>
<div style="float:center;max-width:90%; max-height: 90%;" markdown="1">
<p><img src="images/residuos5.JPG" /></p>
</div>
<p>Por fim e não menos importante, devemos checar a normalidade dos resíduos. Isso pode ser feito por diversos gráficos: gráfico da distribuição (Box plot, histograma, gráfico de pontos ou gráfico de ramos e folhas); gráfico de comparação de frequências; gráfico de normalidade dos resíduos (o mais usado).</p>
</div>
<div id="correlação" class="section level1">
<h1>1.3.1 Correlação</h1>
<p>As relações entre as variáveis ajustadas no modelo podem ser fortes ou fracas e avaliar essa força é muito importante para a avaliação do modelo. Uma forma de quantificar essa força é pelo <strong>correlação</strong>, denotada pela letra R. Suponha que temos um conjunto de dados <span class="math inline">\((x_1,y_1),(x_2,y_2),...,(x_n,y_n)\)</span>, sua correlação é dada por:</p>
<p><span class="math display">\[R = \frac{1}{n-1}\sum_{i=1}^{n} \frac{x_i-\bar{x}}{s_x} \frac{y_i-\bar{y}}{s_y}\]</span> onde <span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(\bar{y}\)</span>, <span class="math inline">\({s_x}\)</span> e <span class="math inline">\({s_y}\)</span> são respectivamente as médias e desvio padrão amostral das variáveis x e y.</p>
<p>Os valores de R serão sempre entre -1 e 1. Quanto mais próximo de 1, mais forte é a correlação linear positiva entre as variáveis. E quanto mais próximo de -1, maior é a correlação linear negativa entre as mesmas. Abaixo temos a imagem que representa as duas situações: em (a) temos três casos de correlação linear positiva e em (b) temos os casos para correlação linear negativa.</p>
<div style="float:center;max-width:90%; max-height: 90%;" markdown="1">
<p><img src="images/residuos3.png" /></p>
</div>
</div>
<div id="coeficiente-de-determinação" class="section level1">
<h1>1.3.1 Coeficiente de Determinação</h1>
<p>R² ajustado</p>
</div>
<div id="análise-de-variância" class="section level1">
<h1>Análise de Variância</h1>
<p>Applied regresion pag 52,65-68 Applied Linear Statistical Models 5th pag 87-97</p>
<p>a)pq fazer b)exemplos c)como fazer</p>
<p>Applied Linear Statistical Models 5th pag 98</p>
<p>Applied regresion pag 55/64-65</p>
<div id="intervalos-de-confiança" class="section level3">
<h3>Intervalos de confiança</h3>
<p>Applied Linear Statistical Models 5th pag 68,73-78</p>
<p>Applied regression pag 58,59</p>
<pre><code>  a)  exemplos de uso
  b) como fazer</code></pre>
</div>
<div id="teste-de-hipótese" class="section level3">
<h3>Teste de Hipótese</h3>
<pre><code>  a)  exemplos de uso
  b) como fazer</code></pre>
</div>
<div id="predição" class="section level3">
<h3>Predição</h3>
<pre><code>  a)  exemplos de uso
  b) como fazer</code></pre>
</div>
<div id="diagnostico" class="section level3">
<h3>Diagnostico</h3>
<p>a)pq fazer b)como fazer ### Transformações a)pq fazer b)exemplos c)como fazer</p>
<p> </p>
<hr />
<p>+999999999 ## Regressão Linear Multipla</p>
</div>
<div id="forma-matricial" class="section level3">
<h3>Forma matricial</h3>
</div>
<div id="estimação-dos-parâmetros-1" class="section level3">
<h3>Estimação dos parâmetros</h3>
</div>
<div id="análise-de-variância-1" class="section level3">
<h3>Análise de Variância</h3>
</div>
<div id="predição-1" class="section level3">
<h3>Predição</h3>
</div>
<div id="teste-de-hipótese-1" class="section level3">
<h3>Teste de Hipótese</h3>
</div>
<div id="intervalo-de-confiança" class="section level3">
<h3>Intervalo de Confiança</h3>
<p>Soma extra de quadrados(?)<br />
Coef de determinação parcial (?)<br />
Reg multi padronizada (?)<br />
Multicolineariedade(?)<br />
Medidas de influência e Alavancagem (?) *Caio</p>
<p> </p>
</div>
<div id="regressão-polinomial" class="section level2">
<h2>Regressão Polinomial</h2>
<p>Regressão com intervenção (?)<br />
Região de confiança (?)<br />
Teste de Hip Linear (?)</p>
<p> </p>
</div>
<div id="seleção-de-modelos" class="section level2">
<h2>Seleção de modelos</h2>
<p>Regressão Parcial (?)<br />
Inflação da variância (?)</p>
<p> </p>
</div>
<div id="regressão-não-linear" class="section level2">
<h2>Regressão Não-Linear</h2>
<p>Regressão logistica(?) *openintro</p>
</div>
</div>
<div id="referência" class="section level1">
<h1>Referência</h1>
<p> </p>
<ul>
<li>Análise de Regressão</li>
</ul>
<p><strong>Azevedo, Caio L. N.</strong>. ME 613A - Análise de regressão, Primeiro Semestre 2019. IMECC - UNICAMP. Disponível em: <a href="https://www.ime.unicamp.br/~cnaber/Material_ME613_1S_2019.htm">https://www.ime.unicamp.br/~cnaber/Material_ME613_1S_2019.htm</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Carvalho, B</strong>. ME613: Análise de Regressão. Github. 2021. Disponível em: <a href="http://me613-unicamp.github.io/">http://me613-unicamp.github.io/</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Caffo, B</strong>. (2019). Regression models for data science in R. Leanpub. 2019. Disponível em: <a href="https://leanpub.com/regmods/read">https://leanpub.com/regmods/read</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Diez, D.; Rundel, M. C.; Barr, C. D.</strong>. Openintro Statistics. Quarta edição. Atualizado em 12 de Novembro de 2019. Versão online gratuita. Disponível em: <a href="https://www.openintro.org/book/os/">https://www.openintro.org/book/os/</a>. Acessado em: Julho à X de 2021</p>
<p><strong>Kutner, M. H.; Nachtsheim, C. J.;Neter, J.; Li, W.</strong>.Applied Linear Statistical Models. Quinta edição. 2005.</p>
<p>João L. F. Batista. Análise de Regressão Aplicada.2004. Departamento de Ciências Florestais ESALQ - USP</p>
<p><a href="http://www.leg.ufpr.br/~paulojus/embrapa/Rembrapa/Rembrapase19.html" class="uri">http://www.leg.ufpr.br/~paulojus/embrapa/Rembrapa/Rembrapase19.html</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
